{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2caab6-d463-4e13-9dbb-a2e868de6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import transformers as ts\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a53f82-115d-4cff-9648-0799f2007e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "def set_seed(seed = 42): \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b7d72-7db5-4cb9-a86c-42e2650a8c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modo = 'uguale' # or \"assoluto\" uguale means taking the same number of examples from each class\n",
    "# soglia = 0.8\n",
    "# do_mapping = False\n",
    "# num_epochs = 10\n",
    "# data = pd.DataFrame([[]], columns=['text','labels','prob'])\n",
    "\n",
    "# Training of the distilbert model using labels from the bart zero-shot classification in 1_step.ipynb\n",
    "\n",
    "def ZS_self_lrn(data, soglia = 0.9, modo = 'assoluto', \n",
    "                num_epochs = 10, batch_size = 5, do_mapping = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Performs zero-shot self-learning on a labeled dataset using DistilBERT.\n",
    "\n",
    "    Parameters:\n",
    "    - data (DataFrame): A pandas DataFrame with at least 'text', 'labels', and 'prob' columns.\n",
    "    - soglia (float): Threshold for selecting high-confidence samples.\n",
    "                      In 'assoluto' mode, samples with prob >= soglia are selected.\n",
    "                      In 'uguale' mode, determines sample proportion per class.\n",
    "    - modo (str): Selection mode. \n",
    "                  'assoluto' selects samples based on absolute probability threshold.\n",
    "                  'uguale' selects an equal number of top samples from each class.\n",
    "    - num_epochs (int): Number of training epochs\n",
    "    - batch_size (int): Training batch size \n",
    "    - do_mapping (bool): If True, maps string labels to integers and modifies the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    set_seed()\n",
    "\n",
    "    labels = data.labels.value_counts().index.to_list()   # pandas\n",
    "    if do_mapping:\n",
    "        mapping = dict()\n",
    "        for i,el in enumerate(labels):\n",
    "            mapping[el] = i\n",
    "        data['labels'] = data['labels'].map(mapping)   # pandas\n",
    "        print(mapping)\n",
    "    \n",
    "    n_labels = len(labels)\n",
    "    if modo == 'uguale':\n",
    "        conta = int(data.shape[0]*(1-soglia)/n_labels)   # pandas\n",
    "        sottoins = []\n",
    "        for el in range(n_labels):\n",
    "            s_ins = data.loc[data.labels==el,:].sort_values('prob', ascending=False)   # pandas\n",
    "            sottoins.append(s_ins.head(conta)[['text','labels']])   # pandas\n",
    "        dataset = pd.concat(sottoins, axis=0)   # pandas\n",
    "    elif modo == 'assoluto':\n",
    "        dataset = data.loc[data.prob >= soglia,['text','labels']]   # pandas\n",
    "        \n",
    "    dati = Dataset.from_pandas(dataset)   # pandas\n",
    "    try:\n",
    "        dati = dati.remove_columns('__index_level_0__')   # pandas\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # we use distilbert and its tokenizer as discussed in the paper\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert/distilbert-base-uncased\", num_labels=n_labels)\n",
    "    \n",
    "    # training loop. we track the loss function values if the user wants to check it\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "        # return tokenizer(examples[\"text\"], padding=True, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_text = dati.map(tokenize_function, batched=True)\n",
    "    tokenized_text = tokenized_text.remove_columns(\"text\")\n",
    "    tokenized_text.set_format(\"torch\")\n",
    "    text_loader = DataLoader(tokenized_text, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    num_training_steps = num_epochs * len(text_loader)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    f_loss = nn.CrossEntropyLoss()\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    model.train()\n",
    "    loss_l = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in enumerate(text_loader):\n",
    "            labels = batch.pop('labels')\n",
    "            labels = labels.to(device)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = f_loss(outputs.logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            if i%30==0:\n",
    "                loss_l.append(loss.item())\n",
    "    \n",
    "    # Use these to save and load the finetuned model\n",
    "    # model.save_pretrained(your_path, from_pt=True)\n",
    "    # model=AutoModelForSequenceClassification.from_pretrained(your_path, num_labels=n_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47da04-8228-43d4-98d1-8cba9977672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lunghezza del dataset completo senza [deleted] nei testi è 25528\n",
      "La lunghezza del dataset con labels sicure oltre il 90% è 6061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "labels\n",
       "Positive    50.371226\n",
       "Negative    49.628774\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additional filter to delete submission that contains the string [deleted], \n",
    "# since it corresponds to deleted posts at the moment of data collection\n",
    "\n",
    "#load dataset\n",
    "data_pandas_prova=pd.read_csv(r'C:\\Tempor\\Reddit\\Pos_Neg\\df_pandas.csv')\n",
    "data_pandas_prova=data_pandas_prova.drop(labels=['Unnamed: 0'], axis=1)\n",
    "\n",
    "#delete all records with text deleted\n",
    "data_pandas_prova=data_pandas_prova[data_pandas_prova['text']!='[deleted]']\n",
    "print('Length of dataset without [deleted] users messages: '+ str(len(data_pandas_prova)))\n",
    "\n",
    "# counts how many records have high-condifence labels (over 90%) and checks if the dataset is balanced\n",
    "data_pandas_check=data_pandas_prova[data_pandas_prova['prob']>0.9]\n",
    "print('Length of dataset containing high-confidence texts: '+ str(len(data_pandas_check)))\n",
    "val_count = data_pandas_check['labels'].value_counts()\n",
    "percentage=val_count*100/len(data_pandas_check)\n",
    "\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.bar(val_count.index, percentage.values)\n",
    "# plt.title(\"Sentiment Data Distribution\")\n",
    "\n",
    "num_labels=[]\n",
    "for line in data_pandas_prova['labels']:\n",
    "    if(line=='Positive'):\n",
    "        num_labels.append(0)\n",
    "    else:\n",
    "        num_labels.append(1)\n",
    "\n",
    "num_labels_series=pd.Series(num_labels)\n",
    "data_pandas=data_pandas_prova.drop(['labels'], axis=1)\n",
    "data_pandas['labels']=num_labels_series.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411967b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZS_self_lrn(data_pandas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
