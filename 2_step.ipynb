{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2caab6-d463-4e13-9dbb-a2e868de6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import transformers as ts\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a53f82-115d-4cff-9648-0799f2007e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "def set_seed(seed = 42): \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b7d72-7db5-4cb9-a86c-42e2650a8c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modo = 'uguale'\n",
    "# soglia = 0.8\n",
    "# do_mapping = False\n",
    "# num_epochs = 10\n",
    "# data = pd.DataFrame([[]], columns=['text','labels','prob'])\n",
    "\n",
    "# Training of the distilbert model using labels from the bart zero-shot classification in 1_step.ipynb\n",
    "\n",
    "def ZS_self_lrn(data, soglia = 0.9, modo = 'assoluto', \n",
    "                num_epochs = 10, batch_size = 5, do_mapping = False):\n",
    "\n",
    "    set_seed()\n",
    "\n",
    "    labels = data.labels.value_counts().index.to_list()   # pandas\n",
    "    if do_mapping:\n",
    "        mapping = dict()\n",
    "        for i,el in enumerate(labels):\n",
    "            mapping[el] = i\n",
    "        data['labels'] = data['labels'].map(mapping)   # pandas\n",
    "        print(mapping)\n",
    "    \n",
    "    n_labels = len(labels)\n",
    "    if modo == 'uguale':\n",
    "        conta = int(data.shape[0]*(1-soglia)/n_labels)   # pandas\n",
    "        sottoins = []\n",
    "        for el in range(n_labels):\n",
    "            s_ins = data.loc[data.labels==el,:].sort_values('prob', ascending=False)   # pandas\n",
    "            sottoins.append(s_ins.head(conta)[['text','labels']])   # pandas\n",
    "        dataset = pd.concat(sottoins, axis=0)   # pandas\n",
    "    elif modo == 'assoluto':\n",
    "        dataset = data.loc[data.prob >= soglia,['text','labels']]   # pandas\n",
    "        \n",
    "    dati = Dataset.from_pandas(dataset)   # pandas\n",
    "    try:\n",
    "        dati = dati.remove_columns('__index_level_0__')   # pandas\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert/distilbert-base-uncased\", num_labels=n_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "        # return tokenizer(examples[\"text\"], padding=True, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_text = dati.map(tokenize_function, batched=True)\n",
    "    tokenized_text = tokenized_text.remove_columns(\"text\")\n",
    "    tokenized_text.set_format(\"torch\")\n",
    "    text_loader = DataLoader(tokenized_text, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    num_training_steps = num_epochs * len(text_loader)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    f_loss = nn.CrossEntropyLoss()\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    model.train()\n",
    "    loss_l = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in enumerate(text_loader):\n",
    "            labels = batch.pop('labels')\n",
    "            labels = labels.to(device)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = f_loss(outputs.logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            if i%30==0:\n",
    "                loss_l.append(loss.item())\n",
    "    \n",
    "    model.save_pretrained(r'C:\\Tempor\\reddit\\Classification_Model', from_pt=True)\n",
    "\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(r'C:\\Tempor\\reddit\\Classification_Model', num_labels=n_labels)\n",
    "\n",
    "    # del tokenized_text,batch,loss,dati\n",
    "    # dati = Dataset.from_pandas(data[['text','labels']])   # pandas\n",
    "    # try:\n",
    "    #     dati = dati.remove_columns('__index_level_0__')   # colpa (a volte) di pandas\n",
    "    # except:\n",
    "    #     pass\n",
    "    \n",
    "    # tokenized_text = dati.map(tokenize_function, batched=True)\n",
    "    # tokenized_text = tokenized_text.remove_columns(\"text\")\n",
    "    # tokenized_text.set_format(\"torch\")\n",
    "    # text_loader = DataLoader(tokenized_text, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "    \n",
    "    # num_test_steps = num_epochs * len(text_loader)\n",
    "    # tqdm._instances.clear()\n",
    "    # progress_bar = tqdm(range(num_test_steps))\n",
    "    \n",
    "    # preds = []\n",
    "    # model.eval()\n",
    "    # for batch in text_loader:\n",
    "    #     labels = batch.pop('labels')\n",
    "    #     labels = labels.to(device)\n",
    "    #     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    #     with torch.no_grad():\n",
    "    #         outputs = model(**batch)\n",
    "    #     logits = outputs.logits\n",
    "    #     predictions = torch.argmax(logits, dim=-1)\n",
    "    #     preds.append(predictions.cpu().numpy())\n",
    "    #     progress_bar.update(1)\n",
    "    # preds = np.hstack(preds)\n",
    "    \n",
    "    # return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47da04-8228-43d4-98d1-8cba9977672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lunghezza del dataset completo senza [deleted] nei testi è 25528\n",
      "La lunghezza del dataset con labels sicure oltre il 90% è 6061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "labels\n",
       "Positive    50.371226\n",
       "Negative    49.628774\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additional filter to delet submission that contains the string [deleted], \n",
    "# since it corresponds to deleted posts at the moment of data collection\n",
    "\n",
    "#load dataset\n",
    "data_pandas_prova=pd.read_csv(r'C:\\Tempor\\Reddit\\Pos_Neg\\df_pandas.csv')\n",
    "data_pandas_prova=data_pandas_prova.drop(labels=['Unnamed: 0'], axis=1)\n",
    "\n",
    "#delete all records with text deleted\n",
    "data_pandas_prova=data_pandas_prova[data_pandas_prova['text']!='[deleted]']\n",
    "print('La lunghezza del dataset completo senza [deleted] nei testi è '+ str(len(data_pandas_prova)))\n",
    "\n",
    "#conta quanti records hanno labels sicure oltre il 90% e controlla se il dataset è bilanciato\n",
    "data_pandas_check=data_pandas_prova[data_pandas_prova['prob']>0.9]\n",
    "print('La lunghezza del dataset con labels sicure oltre il 90% è '+ str(len(data_pandas_check)))\n",
    "val_count = data_pandas_check['labels'].value_counts()\n",
    "percentage=val_count*100/len(data_pandas_check)\n",
    "\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.bar(val_count.index, percentage.values)\n",
    "# plt.title(\"Sentiment Data Distribution\")\n",
    "\n",
    "#mettiamo dei valori numerici alle label: 0:Positive, 1:Negative\n",
    "num_labels=[]\n",
    "for line in data_pandas_prova['labels']:\n",
    "    if(line=='Positive'):\n",
    "        num_labels.append(0)\n",
    "    else:\n",
    "        num_labels.append(1)\n",
    "\n",
    "num_labels_series=pd.Series(num_labels)\n",
    "data_pandas=data_pandas_prova.drop(['labels'], axis=1)\n",
    "data_pandas['labels']=num_labels_series.values\n",
    "\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411967b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pandas_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834cf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la lunghezza del dataframe di submissions di apple filtrato è 159\n",
      "la lunghezza del dataframe di submissions di amazon filtrato è 322\n",
      "la lunghezza del dataframe di submissions di google filtrato è 223\n"
     ]
    }
   ],
   "source": [
    "# text=data_pandas_check['text']\n",
    "# x=np.array(text.str.contains('|'.join(['Apple','APPLE','AAPL','aapl'])) )\n",
    "# y=np.array(text.str.contains('|'.join(['Amazon','AMAZON','AMZN','amzn'])) )\n",
    "# z=np.array(text.str.contains('|'.join(['Google', 'GOOGLE','GOOG','goog'])) )\n",
    "\n",
    "# df_sub_wsb_apple=data_pandas_check[x]\n",
    "# df_sub_wsb_amazon=data_pandas_check[y]\n",
    "# df_sub_wsb_google=data_pandas_check[z]\n",
    "# print('la lunghezza del dataframe di submissions di apple filtrato è ' + str(len(df_sub_wsb_apple)))\n",
    "# print('la lunghezza del dataframe di submissions di amazon filtrato è ' + str(len(df_sub_wsb_amazon)))\n",
    "# print('la lunghezza del dataframe di submissions di google filtrato è ' + str(len(df_sub_wsb_google)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9501e44f-06a6-432a-9ac2-311057a3cca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07fb3c7b55b4293a0943272f003b709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12130/12130 [31:03:12<00:00,  9.22s/it]\n"
     ]
    }
   ],
   "source": [
    "ZS_self_lrn(data_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c4ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
