{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedbfbc6-482a-41f9-a65f-c0480bbe0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45585ee0-7f12-4dbd-8b81-3bf75ee0f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devo creare un dataframe con tre colonne: num_settimana, num_pos, num_neg\n",
    "\n",
    "# \\ num settimana \\ num pos \\ num neg \\ \n",
    "# \\---------------\\---------\\---------\\\n",
    "# \\-----2---------\\----20---\\---80----\\\n",
    "# \\---------------\\---------\\---------\\\n",
    "# \\---------------\\---------\\---------\\\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb15748f-7a23-4dad-a085-a684096a39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple\\2015_1_1.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ceb2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subreddit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subreddit_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_utc",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sentiment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sentiment_BART",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e50abc36-0de4-4b4e-9652-c1048d7506c9",
       "rows": [
        [
         "0",
         "2qyv3n",
         "RaymondDuke",
         "One of the \"secrets\" to successful positioning is to showcase your product (or service) as something **new** (even if it's not really new -- I'm looking at you, Apple). \n\nI call this a \"secret\" (note: the quotes) because it's one of those obvious marketing techniques that is easily overlooked by most entrepreneurs.\n\nSo what exactly do I mean about positioning your product as something new?\n\nLet me explain...\n\nWhen you market your product, it's more important that you appeal to the **desires of the market** than the *features* of your product. \n\nWhat I mean is, the actual \"thing\" your product does is secondary to the emotional reasons why people will buy it. \n\nNow there are all kinds of \"emotional triggers\" that will get people to buy.\n\n* Lust (i.e., and it's not *just* sex -- it could be the physical desire for an object, like a new car or house. Lusting for objects is often (but not always) linked with power and or wanting control).\n\n* Fear (i.e., avoiding a pain, problem, and or unwanted circumstance. People often buy into coaching, training, or other 'leadership' services to avoid the fear of failure of doing it alone).\n\n* Escape (i.e., there's greener pastures on the 'other side.')\n\nThe last emotion -- escape -- is where the concept of newness comes in, so it's the one I'm now going to talk about for the rest of this post.\n\n**The #1 positioning mistake most entrepreneurs make** \n\nThe mistake (as I see it) is new products by entrepreneurs don't market the number one asset they have: the fact that they're new. \n\nThis is a mistake because newness is a very powerful psychological trigger -- people will *always* want something new in their lives. Our brains are hard-wired to the concept of newness from the moment we're born. \n\nHell, even the cycle of life is based on things that are \"new.\"\n\nA new day...a new meal...a new meeting...a new post on your Facebook feed. \n\nAnd so on. \n\nLet me explain this another way, because this is an easy concept to understand. I don't want to make this confusing at all, because it's not. \n\nThe desire (and expectation) for something new is a thing that exists in every human being. \n\nThink of this desire as a current in a stream. \n\nTo best position your product in a way that gains momentum, position your product as something new to \"ride\" the stream (which is way easier and less stressful than swimming against the current). \n\nAgain, it comes to escaping -- discovering a **new** way of living life. \n\nThis concept of newness isn't new; I've just positioned it that way. That's what I do as a copywriter -- I take products (or services) that people make, and position them in the market in a way that's different from similar solutions in the market. \n\nI could go on and on about this, but I'm thinking it'd be better to open this up for discussion. \n\nGo ahead and leave a comment if you think this is interesting. \n\nOr...if you'd like feedback on how to position YOUR product in a new way, we can do that too. \n\nJust drop the comment below, and I'll get back to you soon.\n\n:-)\n\n**edit -- 01/01/2015**: Happy New Years! I'm back here for an hour or so. If I don't reply, it doesn't mean I never will. As long as people comment, I'll be responding. ",
         "t3_2qyv3n",
         "Entrepreneur",
         "t5_2qldo",
         "2015-01-01 01:28:14",
         "1",
         "1",
         "0"
        ],
        [
         "1",
         "2qzy28",
         "gsstechnology",
         "CV PC Repair - PC, Laptop and Apple Mac Repair in Coventry &amp; Warwickshire",
         "t3_2qzy28",
         "business",
         "t5_2qgzg",
         "2015-01-01 10:55:36",
         "1",
         "0",
         "0"
        ],
        [
         "2",
         "2r0ljh",
         "OnionMan69",
         "Apple sued over iPhone, iPad storage. Ever wonder why there never is enough space on your iPhone or iPad? Upgrades to iOS seem to take up as much as 23% or ~1.3 GB of the storage space on their devices. Meanwhile Apple aggressively markets its fee-based iCloud storage system.",
         "t3_2r0ljh",
         "business",
         "t5_2qgzg",
         "2015-01-01 17:59:06",
         "1",
         "1",
         "1"
        ],
        [
         "3",
         "2r16ay",
         "M0neyTrees",
         "How do you think Apple's share price will evolve this year? The share price went up a lot in 2014, but has it reached it's top? \n\nMy portfolio is currently only Norwegian stocks, but I want to invest in the US Stock Marked as it is more stable. Any other suggestion for a semi-big investment in the US Stock Marked?\n\nThanks :)",
         "t3_2r16ay",
         "investing",
         "t5_2qhhq",
         "2015-01-01 21:27:27",
         "1",
         "0",
         "1"
        ],
        [
         "4",
         "2r332t",
         "futuremarketinsight",
         "Chinese smartphone startup Xiaomi recently made headlines thanks to the tremendous success of their Redmi Note 4G model. Put up on a flash sale on Indian ecommerce website Flipkart, all 40,000 handsets sold out in six seconds! According to an FMI analyst, Samsung, LG, Lenovo and Apple particularly, rely on the sales of their high-end handsets. In fact, it was the unprecedented sales of LG’s low-end mobiles this year that helped them to achieve high sales. 2014 was the first time LG’s annual sales exceeded 15 million handsets.",
         "t3_2r332t",
         "business",
         "t5_2qgzg",
         "2015-01-02 09:26:26",
         "1",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>week</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_BART</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2qyv3n</td>\n",
       "      <td>RaymondDuke</td>\n",
       "      <td>One of the \"secrets\" to successful positioning...</td>\n",
       "      <td>t3_2qyv3n</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>t5_2qldo</td>\n",
       "      <td>2015-01-01 01:28:14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2qzy28</td>\n",
       "      <td>gsstechnology</td>\n",
       "      <td>CV PC Repair - PC, Laptop and Apple Mac Repair...</td>\n",
       "      <td>t3_2qzy28</td>\n",
       "      <td>business</td>\n",
       "      <td>t5_2qgzg</td>\n",
       "      <td>2015-01-01 10:55:36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2r0ljh</td>\n",
       "      <td>OnionMan69</td>\n",
       "      <td>Apple sued over iPhone, iPad storage. Ever won...</td>\n",
       "      <td>t3_2r0ljh</td>\n",
       "      <td>business</td>\n",
       "      <td>t5_2qgzg</td>\n",
       "      <td>2015-01-01 17:59:06</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2r16ay</td>\n",
       "      <td>M0neyTrees</td>\n",
       "      <td>How do you think Apple's share price will evol...</td>\n",
       "      <td>t3_2r16ay</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "      <td>2015-01-01 21:27:27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2r332t</td>\n",
       "      <td>futuremarketinsight</td>\n",
       "      <td>Chinese smartphone startup Xiaomi recently mad...</td>\n",
       "      <td>t3_2r332t</td>\n",
       "      <td>business</td>\n",
       "      <td>t5_2qgzg</td>\n",
       "      <td>2015-01-02 09:26:26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id               author  \\\n",
       "0  2qyv3n          RaymondDuke   \n",
       "1  2qzy28        gsstechnology   \n",
       "2  2r0ljh           OnionMan69   \n",
       "3  2r16ay           M0neyTrees   \n",
       "4  2r332t  futuremarketinsight   \n",
       "\n",
       "                                                text       name     subreddit  \\\n",
       "0  One of the \"secrets\" to successful positioning...  t3_2qyv3n  Entrepreneur   \n",
       "1  CV PC Repair - PC, Laptop and Apple Mac Repair...  t3_2qzy28      business   \n",
       "2  Apple sued over iPhone, iPad storage. Ever won...  t3_2r0ljh      business   \n",
       "3  How do you think Apple's share price will evol...  t3_2r16ay     investing   \n",
       "4  Chinese smartphone startup Xiaomi recently mad...  t3_2r332t      business   \n",
       "\n",
       "  subreddit_id          created_utc  week  Sentiment  Sentiment_BART  \n",
       "0     t5_2qldo  2015-01-01 01:28:14     1          1               0  \n",
       "1     t5_2qgzg  2015-01-01 10:55:36     1          0               0  \n",
       "2     t5_2qgzg  2015-01-01 17:59:06     1          1               1  \n",
       "3     t5_2qhhq  2015-01-01 21:27:27     1          0               1  \n",
       "4     t5_2qgzg  2015-01-02 09:26:26     1          0               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e50caf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12776"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(numero_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a715566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6350"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(numero_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590b8032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['created_utc'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858da6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=df.filter([\"author\",\"Sentiment\"])\n",
    "dd=dff.groupby('author').mean().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04128711-3199-46e7-9f5c-f816983d3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_settimana=[]\n",
    "pos=[]\n",
    "num_pos=[]\n",
    "neg=[]\n",
    "num_neg=[]\n",
    "err=[]\n",
    "numero_dati=[]\n",
    "numero_comments=[]\n",
    "numero_submissions=[]\n",
    "tot_post=[]\n",
    "\n",
    "for i in range(2,6):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple\\2015_1_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "    # pos=df[df['Sentiment']==0]\n",
    "    # neg=df[df['Sentiment']==1]\n",
    "    # num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    # num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "\n",
    "for i in range(6,10):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple\\2015_2_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(10,14):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple\\2015_3_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(14,19):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple\\2015_4_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(19,23):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2015_5_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(23,27):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2015_6_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(27,32):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2015_7_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(32,36):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2015_8_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(36,41):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2015_9_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(41,45):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2015_10_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(45,49):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2015_11_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(49,54):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2015_12_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0])\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(1,5):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2016_1_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0]+53)\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(5,9):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2016_2_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0]+53)\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(9,14):\n",
    "    df=pd.read_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple_a\\2016_3_'+str(i)+'.json')\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0]+53)\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(14,18):\n",
    "    df=pd.read_json(r\"C:\\Tempor\\Reddit\\sentiment_data\\apple_a\\2016_4_\"+str(i)+\".json\")\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0]+53)\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(18,22):\n",
    "    df=pd.read_json(r\"C:\\Tempor\\Reddit\\sentiment_data\\apple_a\\2016_5_\"+str(i)+\".json\")\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0]+53)\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "\n",
    "for i in range(22,25):\n",
    "    df=pd.read_json(r\"C:\\Tempor\\Reddit\\sentiment_data\\apple_a\\2016_6_\"+str(i)+\".json\")\n",
    "    a = df\n",
    "    b = a.loc[np.logical_not(a.text.str.contains('gift card'))]\n",
    "    c = b.loc[b.text.str.replace(\"\\d+\", ' ', regex=True).str.replace('\\n',' ').apply(lambda x: ' '.join(filter(lambda word: len(word) >= 2, x.split()))).apply(lambda x: len(x.split()) >= 10),:]\n",
    "    d = c.loc[np.logical_not(np.logical_and(c.text.str.lower().str.contains('pc'),c.text.str.lower().str.contains('repair'))),:]\n",
    "    d=d.filter([\"author\",\"Sentiment\"])\n",
    "    d=d[d[\"author\"]!=\"[deleted]\"]\n",
    "    d=d.groupby('author').mean().astype(int)\n",
    "    num_settimana.append(df['week'][0]+53)\n",
    "    pos=d[d['Sentiment']==0]\n",
    "    neg=d[d['Sentiment']==1]\n",
    "    num_pos.append(len(pos)/(len(pos)+len(neg)))\n",
    "    num_neg.append(len(neg)/(len(pos)+len(neg)))\n",
    "    err.append((1/(len(pos)+len(neg)))*np.sqrt(len(pos)*len(neg)/(len(pos)+len(neg))))\n",
    "    numero_dati.append(len(pos)+len(neg))\n",
    "    num_com=len(df[df['name'].str.startswith('t1')])\n",
    "    num_sub=len(df[df['name'].str.startswith('t3')])\n",
    "    numero_comments.append(num_com)\n",
    "    numero_submissions.append(num_sub)\n",
    "    tot_post.append(len(df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2670004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(numero_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66ec567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(numero_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43b1e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(tot_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c88ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Totale dati\n",
    "# G1oogle: 56494\n",
    "# A1pple: 18948\n",
    "# A1mazon: 67447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43224cbf-124a-4615-b251-fce143598079",
   "metadata": {},
   "outputs": [],
   "source": [
    "dizionario={'num_settimana': num_settimana, 'pos': num_pos, 'neg': num_neg}\n",
    "df_plot=pd.DataFrame(data=dizionario)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(df_plot['num_settimana'], df_plot['neg'],'o-', label='apple')\n",
    "# plt.errorbar(df_plot['num_settimana'], df_plot['neg'], yerr=err, fmt='o-')\n",
    "# plt.plot(sentiment_apple, 'o-', label='apple')\n",
    "# plt.plot(sentiment_apple, 'o-', label='apple')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.ylabel('Values')\n",
    "plt.xlabel('Number of week')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0861625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plot.to_json(r'C:\\Tempor\\reddit\\sentiment_data\\apple\\apple_DistilBERT.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e4bd707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_settimana</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.603774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.634146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.385507</td>\n",
       "      <td>0.614493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.341991</td>\n",
       "      <td>0.658009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>73</td>\n",
       "      <td>0.384375</td>\n",
       "      <td>0.615625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>74</td>\n",
       "      <td>0.371528</td>\n",
       "      <td>0.628472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>75</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.672727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>76</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>77</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_settimana       pos       neg\n",
       "0               2  0.396226  0.603774\n",
       "1               3  0.365854  0.634146\n",
       "2               4  0.391304  0.608696\n",
       "3               5  0.385507  0.614493\n",
       "4               6  0.341991  0.658009\n",
       "..            ...       ...       ...\n",
       "71             73  0.384375  0.615625\n",
       "72             74  0.371528  0.628472\n",
       "73             75  0.327273  0.672727\n",
       "74             76  0.354167  0.645833\n",
       "75             77  0.285714  0.714286\n",
       "\n",
       "[76 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
